{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T05:41:08.860883800Z",
     "start_time": "2026-01-05T05:41:07.522047900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "os.chdir(\"../\")\n",
    "load_dotenv()\n",
    "\n",
    "from knowledge_graph_creator.llm.llm_inference import LLMInference, LLMConfig, GroqModel\n",
    "from knowledge_graph_creator.extractors.paper_relation_extractor import PaperRelationExtractor\n"
   ],
   "id": "591ddd1010185c7f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asuji\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\knowledge-graph-creator-o7Cxat7M-py3.14\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:26: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T05:41:08.876949900Z",
     "start_time": "2026-01-05T05:41:08.863885600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize LLM client\n",
    "api_key = os.getenv(\"GROP_API_KEY_GRAPH\")\n",
    "llm_config = LLMConfig(model=GroqModel.LLAMA_8B, temperature=0.3)\n",
    "llm_client = LLMInference(api_key=api_key, config=llm_config)\n",
    "\n",
    "# Initialize extractor\n",
    "extractor = PaperRelationExtractor(\n",
    "    uri=os.getenv(\"NEO4J_URI\"),\n",
    "    user=os.getenv(\"NEO4J_USER\"),\n",
    "    password=os.getenv(\"NEO4J_PASSWORD\"),\n",
    "    llm_client=llm_client,\n",
    "    min_delay=1\n",
    ")\n"
   ],
   "id": "df5946a525ffd6df",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T05:41:11.748228800Z",
     "start_time": "2026-01-05T05:41:11.236947300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test 1: Get all triplets from database\n",
    "triplets = extractor.get_all_triplets(min_citation_count=0, min_year=2022)\n",
    "print(f\"Found {len(triplets)} triplets\")\n",
    "if triplets:\n",
    "    print(\"Sample triplet:\", triplets[0])\n"
   ],
   "id": "794a30dbe43019df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 569 triplets\n",
      "Sample triplet: {'tail_id': '85064a4b1b96863af4fccff9ad34ce484945ad7b', 'tail_title': 'Knowledge Graph Embedding: A Survey from the Perspective of Representation Spaces', 'tail_abstract': 'Knowledge graph embedding (KGE) is an increasingly popular technique that aims to represent entities and relations of knowledge graphs into low-dimensional semantic spaces for a wide spectrum of applications such as link prediction, knowledge reasoning and knowledge completion. In this article, we provide a systematic review of existing KGE techniques based on representation spaces. Particularly, we build a fine-grained classification to categorise the models based on three mathematical perspectives of the representation spaces: (1) algebraic perspective, (2) geometric perspective and (3) analytical perspective. We introduce the rigorous definitions of fundamental mathematical spaces before diving into KGE models and their mathematical properties. We further discuss different KGE methods over the three categories, as well as summarise how spatial advantages work over different embedding needs. By collating the experimental results from downstream tasks, we also explore the advantages of mathematical space in different scenarios and the reasons behind them. We further state some promising research directions from a representation space perspective, with which we hope to inspire researchers to design their KGE models as well as their related applications with more consideration of their mathematical space properties.', 'head_id': '6e17c6d0491342e040da8f9c7c6aa7ce0b9cd696', 'head_title': 'Varieties of Representations', 'head_abstract': '<jats:p/>'}\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T05:41:50.962645300Z",
     "start_time": "2026-01-05T05:41:20.503685900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test 2: Extract relation with sample papers (mock data)\n",
    "sample_citing_paper = {\n",
    "    \"title\": \"Attention Is All You Need\",\n",
    "    \"abstract\": \"We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\",\n",
    "}\n",
    "\n",
    "sample_cited_paper = {\n",
    "    \"title\": \"Neural Machine Translation by Jointly Learning to Align and Translate\",\n",
    "    \"abstract\": \"We conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically search for parts of a source sentence that are relevant to predicting a target word.\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "result = extractor.extract_relation_with_structured_llm(\n",
    "    citing_paper=sample_citing_paper,\n",
    "    cited_paper=sample_cited_paper,\n",
    ")\n",
    "\n",
    "print(\"Extraction result:\")\n",
    "if result:\n",
    "    print(result.model_dump())\n",
    "else:\n",
    "    print(\"No result returned\")\n"
   ],
   "id": "856d33efed83049",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction result:\n",
      "{'relationships': [{'type': 'Extends', 'confidence': 'high', 'evidence': 'Paper 1 proposes a new simple network architecture, the Transformer, which is based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Paper 2 introduces the transformer model and conjectures that a fixed-length vector is a bottleneck in improving performance.', 'explanation': 'Paper 1 builds upon and extends the methodology introduced in Paper 2 by proposing a new network architecture based on attention mechanisms.'}, {'type': 'Requires', 'confidence': 'high', 'evidence': 'Paper 2 introduces the transformer model and conjectures that a fixed-length vector is a bottleneck in improving performance. Paper 1 builds upon this model and proposes a new simple network architecture.', 'explanation': 'Paper 1 depends on and builds directly upon the concepts/methods from Paper 2 as a necessary foundation.'}], 'no_relationship_reason': None}\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9f23967147aa49f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T05:44:53.281195500Z",
     "start_time": "2026-01-05T05:44:52.744475400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test 3: Process single triplet from database\n",
    "if triplets:\n",
    "    single_triplet = triplets[0]\n",
    "    citing = {\n",
    "        \"title\": single_triplet[\"tail_title\"],\n",
    "        \"abstract\": single_triplet[\"tail_abstract\"],\n",
    "    }\n",
    "    cited = {\n",
    "        \"title\": single_triplet[\"head_title\"],\n",
    "        \"abstract\": single_triplet[\"head_abstract\"],\n",
    "    }\n",
    "\n",
    "    analysis = extractor.extract_relation_with_structured_llm(citing, cited)\n",
    "    if analysis:\n",
    "        print(\"Relationships found:\")\n",
    "        for rel in analysis.relationships:\n",
    "            print(f\"  - {rel.type} (confidence: {rel.confidence})\")\n",
    "            print(f\"    Evidence: {rel.evidence}\")\n"
   ],
   "id": "d94b5ef24d6ed0f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relationships found:\n",
      "  - Requires (confidence: high)\n",
      "    Evidence: Paper 1 discusses knowledge graph embedding (KGE) and its applications, which is a concept that Paper 2 touches upon in its abstract.\n",
      "  - Adapts-from (confidence: medium)\n",
      "    Evidence: Paper 1 discusses different mathematical perspectives of representation spaces, which is a concept that Paper 2 mentions in its title.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T05:42:47.652896900Z",
     "start_time": "2026-01-05T05:42:47.624729200Z"
    }
   },
   "cell_type": "code",
   "source": "analysis",
   "id": "355fcc3a235f85f0",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T05:45:11.903073900Z",
     "start_time": "2026-01-05T05:45:04.169584300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test 4: Process limited batch (first 3 triplets)\n",
    "limited_triplets = triplets[:3] if len(triplets) >= 3 else triplets\n",
    "\n",
    "results = []\n",
    "import time\n",
    "\n",
    "for idx, triplet in enumerate(limited_triplets):\n",
    "    citing = {\"title\": triplet[\"tail_title\"], \"abstract\": triplet[\"tail_abstract\"]}\n",
    "    cited = {\"title\": triplet[\"head_title\"], \"abstract\": triplet[\"head_abstract\"]}\n",
    "\n",
    "    print(f\"Processing {idx + 1}/{len(limited_triplets)}\")\n",
    "    analysis = extractor.extract_relation_with_structured_llm(citing, cited)\n",
    "\n",
    "    if analysis:\n",
    "        results.append({\n",
    "            \"citing_id\": triplet[\"tail_id\"],\n",
    "            \"cited_id\": triplet[\"head_id\"],\n",
    "            \"relationships\": [r.model_dump() for r in analysis.relationships],\n",
    "        })\n",
    "\n",
    "    time.sleep(2)  # Rate limiting\n",
    "\n",
    "print(f\"\\nProcessed {len(results)} triplets successfully\")\n"
   ],
   "id": "647eb44e85a78736",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1/3\n",
      "Processing 2/3\n",
      "Processing 3/3\n",
      "\n",
      "Processed 3 triplets successfully\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T05:48:41.922743Z",
     "start_time": "2026-01-05T05:48:41.874597400Z"
    }
   },
   "cell_type": "code",
   "source": "limited_triplets[0]",
   "id": "3fb4ab3d93efdd73",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tail_id': '85064a4b1b96863af4fccff9ad34ce484945ad7b',\n",
       " 'tail_title': 'Knowledge Graph Embedding: A Survey from the Perspective of Representation Spaces',\n",
       " 'tail_abstract': 'Knowledge graph embedding (KGE) is an increasingly popular technique that aims to represent entities and relations of knowledge graphs into low-dimensional semantic spaces for a wide spectrum of applications such as link prediction, knowledge reasoning and knowledge completion. In this article, we provide a systematic review of existing KGE techniques based on representation spaces. Particularly, we build a fine-grained classification to categorise the models based on three mathematical perspectives of the representation spaces: (1) algebraic perspective, (2) geometric perspective and (3) analytical perspective. We introduce the rigorous definitions of fundamental mathematical spaces before diving into KGE models and their mathematical properties. We further discuss different KGE methods over the three categories, as well as summarise how spatial advantages work over different embedding needs. By collating the experimental results from downstream tasks, we also explore the advantages of mathematical space in different scenarios and the reasons behind them. We further state some promising research directions from a representation space perspective, with which we hope to inspire researchers to design their KGE models as well as their related applications with more consideration of their mathematical space properties.',\n",
       " 'head_id': '6e17c6d0491342e040da8f9c7c6aa7ce0b9cd696',\n",
       " 'head_title': 'Varieties of Representations',\n",
       " 'head_abstract': '<jats:p/>'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7654594956e57975"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T05:50:14.567967300Z",
     "start_time": "2026-01-05T05:50:13.529531900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test save_relationships with first triplet\n",
    "if limited_triplets and results:\n",
    "    # Use the first triplet and its analysis\n",
    "    first_triplet = limited_triplets[0]\n",
    "\n",
    "    # Re-extract or use existing analysis\n",
    "    citing = {\"title\": first_triplet[\"tail_title\"], \"abstract\": first_triplet[\"tail_abstract\"]}\n",
    "    cited = {\"title\": first_triplet[\"head_title\"], \"abstract\": first_triplet[\"head_abstract\"]}\n",
    "\n",
    "    analysis = extractor.extract_relation_with_structured_llm(citing, cited)\n",
    "\n",
    "    if analysis:\n",
    "        extractor.save_relationships(\n",
    "            citing_id=first_triplet[\"tail_id\"],\n",
    "            cited_id=first_triplet[\"head_id\"],\n",
    "            analysis=analysis\n",
    "        )\n",
    "        print(f\"Saved relationships for {first_triplet['tail_id']} -> {first_triplet['head_id']}\")\n",
    "        print(f\"Relationships: {[r.type for r in analysis.relationships]}\")\n",
    "    else:\n",
    "        print(\"No analysis to save\")"
   ],
   "id": "7017b8e40633f906",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved relationships for 85064a4b1b96863af4fccff9ad34ce484945ad7b -> 6e17c6d0491342e040da8f9c7c6aa7ce0b9cd696\n",
      "Relationships: ['Requires', 'Adapts-from']\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Test 5: Full pipeline with save (use with caution - writes to DB)\n",
    "# Uncomment to run\n",
    "# full_results = extractor.process_all_triplets(min_citation_count=5, min_year=2023)\n",
    "# print(f\"Processed {len(full_results)} triplets\")\n"
   ],
   "id": "3af5239da2bc6c88"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T05:50:59.341901900Z",
     "start_time": "2026-01-05T05:50:59.314300900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cleanup\n",
    "extractor.close()"
   ],
   "id": "3340c27d2c0263c5",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2253fc1c67bc34a7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
